{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 Bucket File Processing\n",
    "\n",
    "This notebook demonstrates how to process all files from an S3 bucket using the Psycore framework's `s3_manager.s3_process_file` function and write the processed results to the `jupyter_testing` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "from datetime import datetime\n",
    "\n",
    "# Add parent directory to path to import the Psycore modules\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Import Psycore modules\n",
    "from src.data.s3_manager import S3Manager\n",
    "from src.data.attachments import Attachment, AttachmentTypes\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"s3_processor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if python-dotenv is installed, if not install it\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "except ImportError:\n",
    "    !pip install python-dotenv\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# S3 Configuration\n",
    "S3_BUCKET_NAME = os.getenv(\"S3_BUCKET_NAME\")\n",
    "S3_PREFIX = os.getenv(\"S3_PREFIX\", \"\")  # Optional prefix to filter objects\n",
    "AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
    "\n",
    "# Processing Configuration\n",
    "MAX_FILES = int(os.getenv(\"MAX_FILES\", \"100\"))  # Max files to process\n",
    "OUTPUT_DIR = os.path.join(os.getcwd(), \"processed_files\")\n",
    "\n",
    "# Check if environment variables are set\n",
    "if not S3_BUCKET_NAME:\n",
    "    S3_BUCKET_NAME = input(\"Enter S3 bucket name: \")\n",
    "    \n",
    "if not AWS_ACCESS_KEY_ID:\n",
    "    AWS_ACCESS_KEY_ID = input(\"Enter AWS Access Key ID (leave blank to use AWS profile): \")\n",
    "    \n",
    "if not AWS_SECRET_ACCESS_KEY and AWS_ACCESS_KEY_ID:\n",
    "    AWS_SECRET_ACCESS_KEY = input(\"Enter AWS Secret Access Key: \")\n",
    "\n",
    "# Print configuration (hiding sensitive values)\n",
    "print(f\"S3 Bucket: {S3_BUCKET_NAME}\")\n",
    "print(f\"S3 Prefix: {S3_PREFIX if S3_PREFIX else 'None (processing all files)'}\")\n",
    "print(f\"AWS Region: {AWS_REGION}\")\n",
    "print(f\"AWS Credentials: {'Provided' if AWS_ACCESS_KEY_ID else 'Using AWS Profile'}\")\n",
    "print(f\"Max Files: {MAX_FILES}\")\n",
    "print(f\"Output Directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize S3 Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize S3 Manager\n",
    "s3_manager = S3Manager(\n",
    "    bucket_name=S3_BUCKET_NAME,\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID if AWS_ACCESS_KEY_ID else None,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY if AWS_SECRET_ACCESS_KEY else None,\n",
    "    aws_region=AWS_REGION\n",
    ")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"S3 Manager initialized for bucket: {S3_BUCKET_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. List Files in S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if tqdm is installed, if not install it\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    !pip install tqdm\n",
    "    from tqdm.notebook import tqdm\n",
    "\n",
    "# List files in the S3 bucket\n",
    "try:\n",
    "    s3_files = s3_manager.list_files(prefix=S3_PREFIX)\n",
    "    print(f\"Found {len(s3_files)} files in the S3 bucket\")\n",
    "    \n",
    "    # Display the first few files\n",
    "    if s3_files:\n",
    "        print(\"\\nSample files:\")\n",
    "        for i, file_info in enumerate(s3_files[:5]):\n",
    "            print(f\"{i+1}. {file_info['key']} ({file_info['size']} bytes)\")\n",
    "        \n",
    "        if len(s3_files) > 5:\n",
    "            print(f\"... and {len(s3_files) - 5} more files\")\n",
    "    else:\n",
    "        print(\"No files found in the bucket with the specified prefix.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error listing files: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_s3_file(file_info: Dict[str, Any], output_dir: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process a single S3 file using s3_manager.s3_process_file\n",
    "    \n",
    "    Args:\n",
    "        file_info: Dictionary with file information\n",
    "        output_dir: Directory to save processed files\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with processing results\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        \"key\": file_info[\"key\"],\n",
    "        \"size\": file_info[\"size\"],\n",
    "        \"success\": False,\n",
    "        \"error\": None,\n",
    "        \"attachment\": None,\n",
    "        \"output_path\": None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Create a subdirectory for this file\n",
    "        file_basename = os.path.basename(file_info[\"key\"])\n",
    "        file_dir = os.path.join(output_dir, file_basename.replace(\".\", \"_\"))\n",
    "        os.makedirs(file_dir, exist_ok=True)\n",
    "        \n",
    "        # Download the file to a temporary directory\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            local_path = s3_manager.download_file(file_info[\"key\"], os.path.join(temp_dir, file_basename))\n",
    "            \n",
    "            if local_path:\n",
    "                # Determine attachment type\n",
    "                attachment_type = AttachmentTypes.from_filename(file_basename)\n",
    "                \n",
    "                # Create and process attachment\n",
    "                attachment = Attachment(\n",
    "                    attachment_type=attachment_type,\n",
    "                    attachment_data=local_path,\n",
    "                    filename=file_basename,\n",
    "                    needsExtraction=True,\n",
    "                    metadata={\"s3_key\": file_info[\"key\"]}\n",
    "                )\n",
    "                \n",
    "                # Extract attachment data\n",
    "                attachment.extract()\n",
    "                \n",
    "                # Save processed data\n",
    "                result[\"attachment\"] = attachment\n",
    "                result[\"success\"] = True\n",
    "                \n",
    "                # Save attachment metadata\n",
    "                metadata_path = os.path.join(file_dir, \"metadata.json\")\n",
    "                \n",
    "                import json\n",
    "                with open(metadata_path, \"w\") as f:\n",
    "                    # Convert metadata to serializable format\n",
    "                    metadata = {\n",
    "                        \"type\": attachment.attachment_type.name,\n",
    "                        \"filename\": attachment.filename,\n",
    "                        \"s3_key\": file_info[\"key\"],\n",
    "                        \"metadata\": attachment.metadata\n",
    "                    }\n",
    "                    json.dump(metadata, f, indent=2, default=str)\n",
    "                \n",
    "                # Save extracted content based on type\n",
    "                if attachment.attachment_type == AttachmentTypes.IMAGE:\n",
    "                    # Save the base64 encoded image\n",
    "                    import base64\n",
    "                    img_data = attachment.attachment_data\n",
    "                    if isinstance(img_data, str) and img_data.startswith(\"data:image\") and \",\" in img_data:\n",
    "                        img_data = img_data.split(\",\", 1)[1]\n",
    "                    if isinstance(img_data, str):\n",
    "                        try:\n",
    "                            img_bytes = base64.b64decode(img_data)\n",
    "                            img_path = os.path.join(file_dir, \"processed_image.jpg\")\n",
    "                            with open(img_path, \"wb\") as f:\n",
    "                                f.write(img_bytes)\n",
    "                            result[\"output_path\"] = img_path\n",
    "                        except Exception as e:\n",
    "                            logger.warning(f\"Could not save processed image: {str(e)}\")\n",
    "                \n",
    "                elif attachment.attachment_type == AttachmentTypes.TEXT:\n",
    "                    # Save the text content\n",
    "                    text_path = os.path.join(file_dir, \"processed_text.txt\")\n",
    "                    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                        f.write(attachment.attachment_data)\n",
    "                    result[\"output_path\"] = text_path\n",
    "                \n",
    "                elif attachment.attachment_type == AttachmentTypes.VIDEO and \"thumbnail\" in attachment.metadata:\n",
    "                    # Save the video thumbnail\n",
    "                    import base64\n",
    "                    thumbnail_data = attachment.metadata.get(\"thumbnail\")\n",
    "                    if thumbnail_data:\n",
    "                        try:\n",
    "                            thumb_bytes = base64.b64decode(thumbnail_data)\n",
    "                            thumb_path = os.path.join(file_dir, \"thumbnail.jpg\")\n",
    "                            with open(thumb_path, \"wb\") as f:\n",
    "                                f.write(thumb_bytes)\n",
    "                            result[\"output_path\"] = thumb_path\n",
    "                        except Exception as e:\n",
    "                            logger.warning(f\"Could not save video thumbnail: {str(e)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        result[\"success\"] = False\n",
    "        result[\"error\"] = str(e)\n",
    "        logger.error(f\"Error processing {file_info['key']}: {str(e)}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Process Files from S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process files with progress bar\n",
    "if s3_files:\n",
    "    # Limit the number of files to process\n",
    "    files_to_process = s3_files[:min(len(s3_files), MAX_FILES)]\n",
    "    print(f\"Processing {len(files_to_process)} files...\")\n",
    "    \n",
    "    # Create a timestamp for this processing run\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = os.path.join(OUTPUT_DIR, f\"run_{timestamp}\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    \n",
    "    # Process each file\n",
    "    results = []\n",
    "    for file_info in tqdm(files_to_process, desc=\"Processing files\"):\n",
    "        result = process_s3_file(file_info, run_dir)\n",
    "        results.append(result)\n",
    "    \n",
    "    # Summarize results\n",
    "    successful = sum(1 for r in results if r[\"success\"])\n",
    "    failed = len(results) - successful\n",
    "    \n",
    "    print(f\"\\nProcessing complete!\")\n",
    "    print(f\"- Successfully processed: {successful}/{len(results)} files\")\n",
    "    print(f\"- Failed: {failed}/{len(results)} files\")\n",
    "    print(f\"- Results saved to: {run_dir}\")\n",
    "    \n",
    "    # Save summary report\n",
    "    import json\n",
    "    summary_path = os.path.join(run_dir, \"summary.json\")\n",
    "    \n",
    "    summary = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"bucket\": S3_BUCKET_NAME,\n",
    "        \"prefix\": S3_PREFIX,\n",
    "        \"total_files\": len(results),\n",
    "        \"successful\": successful,\n",
    "        \"failed\": failed,\n",
    "        \"files\": [{\n",
    "            \"key\": r[\"key\"],\n",
    "            \"size\": r[\"size\"],\n",
    "            \"success\": r[\"success\"],\n",
    "            \"error\": r[\"error\"],\n",
    "            \"output_path\": r[\"output_path\"],\n",
    "            \"type\": r[\"attachment\"].attachment_type.name if r[\"attachment\"] else None\n",
    "        } for r in results]\n",
    "    }\n",
    "    \n",
    "    with open(summary_path, \"w\") as f:\n",
    "        json.dump(summary, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"Summary report saved to: {summary_path}\")\n",
    "else:\n",
    "    print(\"No files to process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze File Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the types of files processed\n",
    "if 'results' in locals() and results:\n",
    "    # Check if matplotlib is installed\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "    except ImportError:\n",
    "        !pip install matplotlib\n",
    "        import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Count file types\n",
    "    file_types = {}\n",
    "    for result in results:\n",
    "        if result[\"attachment\"] and hasattr(result[\"attachment\"], \"attachment_type\"):\n",
    "            file_type = result[\"attachment\"].attachment_type.name\n",
    "            file_types[file_type] = file_types.get(file_type, 0) + 1\n",
    "    \n",
    "    # Display chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(file_types.keys(), file_types.values())\n",
    "    plt.title(\"File Types Processed\")\n",
    "    plt.xlabel(\"File Type\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display type counts\n",
    "    print(\"\\nFile Type Distribution:\")\n",
    "    for file_type, count in file_types.items():\n",
    "        print(f\"- {file_type}: {count} files ({count/len(results)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. View Sample Processed Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a sample of successfully processed files\n",
    "if 'results' in locals() and results:\n",
    "    successful_results = [r for r in results if r[\"success\"] and r[\"output_path\"]]\n",
    "    \n",
    "    if successful_results:\n",
    "        # Try to display different file types\n",
    "        image_result = next((r for r in successful_results \n",
    "                           if r[\"attachment\"] and r[\"attachment\"].attachment_type == AttachmentTypes.IMAGE), None)\n",
    "        text_result = next((r for r in successful_results \n",
    "                          if r[\"attachment\"] and r[\"attachment\"].attachment_type == AttachmentTypes.TEXT), None)\n",
    "        video_result = next((r for r in successful_results \n",
    "                           if r[\"attachment\"] and r[\"attachment\"].attachment_type == AttachmentTypes.VIDEO), None)\n",
    "        \n",
    "        # Display image if available\n",
    "        if image_result and os.path.exists(image_result[\"output_path\"]):\n",
    "            from IPython.display import Image, display\n",
    "            print(f\"Sample processed image: {os.path.basename(image_result['key'])}\")\n",
    "            display(Image(filename=image_result[\"output_path\"]))\n",
    "        \n",
    "        # Display text if available\n",
    "        if text_result and os.path.exists(text_result[\"output_path\"]):\n",
    "            print(f\"\\nSample processed text: {os.path.basename(text_result['key'])}\")\n",
    "            with open(text_result[\"output_path\"], \"r\", encoding=\"utf-8\") as f:\n",
    "                text_content = f.read(1000)  # Read first 1000 chars\n",
    "            print(f\"\\n{text_content}{'...' if len(text_content) >= 1000 else ''}\")\n",
    "        \n",
    "        # Display video thumbnail if available\n",
    "        if video_result and os.path.exists(video_result[\"output_path\"]):\n",
    "            from IPython.display import Image, display\n",
    "            print(f\"\\nSample video thumbnail: {os.path.basename(video_result['key'])}\")\n",
    "            display(Image(filename=video_result[\"output_path\"]))\n",
    "    else:\n",
    "        print(\"No successful file processing results available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create Vector Database from Processed Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have the vector_store module\n",
    "try:\n",
    "    from src.data.vector_store import VectorStore\n",
    "    has_vector_store = True\n",
    "except ImportError:\n",
    "    has_vector_store = False\n",
    "    print(\"Vector store module not found. Skipping vector database creation.\")\n",
    "\n",
    "if has_vector_store and 'results' in locals() and results:\n",
    "    try:\n",
    "        # Check if OPENAI_API_KEY is set\n",
    "        OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not OPENAI_API_KEY:\n",
    "            OPENAI_API_KEY = input(\"Enter your OpenAI API key to create vector embeddings: \")\n",
    "        \n",
    "        if OPENAI_API_KEY:\n",
    "            # Initialize vector store\n",
    "            vector_db_dir = os.path.join(run_dir, \"vector_db\")\n",
    "            vector_store = VectorStore(persist_directory=vector_db_dir, openai_api_key=OPENAI_API_KEY)\n",
    "            \n",
    "            # Add successfully processed attachments to vector store\n",
    "            successful_results = [r for r in results if r[\"success\"] and r[\"attachment\"]]\n",
    "            \n",
    "            print(f\"Adding {len(successful_results)} files to vector database...\")\n",
    "            for result in tqdm(successful_results, desc=\"Creating embeddings\"):\n",
    "                vector_store.add_attachment(result[\"attachment\"])\n",
    "            \n",
    "            print(f\"Vector database created at: {vector_db_dir}\")\n",
    "            \n",
    "            # Perform a test search\n",
    "            query = \"sample search query\"\n",
    "            print(f\"\\nPerforming test search with query: '{query}'\")\n",
    "            search_results = vector_store.search(query, limit=3)\n",
    "            \n",
    "            if search_results:\n",
    "                print(f\"Found {len(search_results)} results:\")\n",
    "                for i, result in enumerate(search_results):\n",
    "                    print(f\"\\nResult {i+1}:\")\n",
    "                    print(f\"Type: {result['store_type']}\")\n",
    "                    print(f\"Content: {result['content'][:100]}...\")\n",
    "                    if 'metadata' in result and 's3_key' in result['metadata']:\n",
    "                        print(f\"Source: {result['metadata']['s3_key']}\")\n",
    "            else:\n",
    "                print(\"No search results found.\")\n",
    "        else:\n",
    "            print(\"OpenAI API key not provided. Skipping vector database creation.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating vector database: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to process files from an S3 bucket using the Psycore framework. The processed files are saved to the `jupyter_testing` directory and can be used for further analysis or loaded into a vector database for semantic search.\n",
    "\n",
    "The processing workflow includes:\n",
    "1. Connecting to an S3 bucket\n",
    "2. Listing files with an optional prefix filter\n",
    "3. Downloading and processing each file based on its type\n",
    "4. Extracting metadata and content\n",
    "5. Saving the processed results\n",
    "6. Creating a vector database for semantic search (if enabled)\n",
    "\n",
    "For production use, consider adding error handling, retries, and parallel processing to improve reliability and performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
